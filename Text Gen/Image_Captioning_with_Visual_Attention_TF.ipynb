{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1tZO_WZOzbt"
      },
      "outputs": [],
      "source": [
        "!apt install --allow-change-held-packages libcudnn8=8.6.0.163-1+cuda11.8\n",
        "!pip uninstall -y tensorflow estimator keras\n",
        "!pip install -U tensorflow_text tensorflow tensorflow_datasets\n",
        "!pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import concurrent.futures\n",
        "import collections\n",
        "import dataclasses\n",
        "import hashlib\n",
        "import itertools\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import pathlib\n",
        "import random\n",
        "import re\n",
        "import string\n",
        "import time\n",
        "import urllib.request\n",
        "\n",
        "import einops\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import requests\n",
        "import tqdm\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "import tensorflow_datasets as tfds"
      ],
      "metadata": {
        "id": "DrR9j2RXPPSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get dataset"
      ],
      "metadata": {
        "id": "ybQP2fRpR2gi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conceptual_captions(*, data_dir='conceptual_captions', num_train, num_val):\n",
        "  def iter_index(index_path):\n",
        "    with open(index_path) as f:\n",
        "      for line in f:\n",
        "        caption, url = line.strip().split('\\t')\n",
        "        yield caption, url\n",
        "\n",
        "  def download_image_urls(data_dir, urls):\n",
        "    ex = concurrent.futures.ThreadPoolExecutor(max_workers=100)\n",
        "    def save_image(url):\n",
        "      hash = hashlib.sha1(url.encode())\n",
        "      # name the files after the hash of the URL\n",
        "      file_path = data_dir/f'{hash.hexdigest()}.jpeg'\n",
        "\n",
        "      if file_path.exists(): return file_path  # download each file only noce\n",
        "\n",
        "      try:\n",
        "        result = requests.get(url, timeout=5)\n",
        "      except Exception:\n",
        "        file_path = None\n",
        "      else:\n",
        "        file_path.write_bytes(result.content)\n",
        "\n",
        "      return file_path\n",
        "\n",
        "    result = []\n",
        "    out_paths = ex.map(save_image, urls)\n",
        "    for file_path in tqdm.tqdm(out_paths, total=len(urls)):\n",
        "      result.append(file_path)\n",
        "\n",
        "    return result\n",
        "\n",
        "  def ds_from_index_file(index_path, data_dir, count):\n",
        "    data_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    index = list(itertools.islice(iter_index(index_path), count))\n",
        "    captions = [caption for caption, url in index]\n",
        "    urls = [url for caption, url in index]\n",
        "\n",
        "    paths = download_image_urls(data_dir, urls)\n",
        "\n",
        "    new_captions = []\n",
        "    new_paths = []\n",
        "    for cap, path in zip(captions, paths):\n",
        "      if path is None:  # download failed for this, skip it\n",
        "        continue\n",
        "\n",
        "      new_captions.append(cap)\n",
        "      new_paths.append(path)\n",
        "\n",
        "    new_paths = [str(p) for p in new_paths]\n",
        "\n",
        "    ds = tf.data.Dataset.from_tensor_slices((new_paths, new_captions))\n",
        "    ds = ds.map(lambda path, cap: (path, cap[tf.newaxis]))  # 1 caption per image\n",
        "\n",
        "    return ds\n",
        "\n",
        "  data_dir = pathlib.Path(data_dir)\n",
        "  train_index_path = tf.keras.utils.get_file(\n",
        "      origin='https://storage.googleapis.com/gcc-data/Train/GCC-training.tsv',\n",
        "      cache_subdir=data_dir,\n",
        "      cache_dir='.'\n",
        "  )\n",
        "  val_index_path = tf.keras.utils.get_file(\n",
        "      origin='https://storage.googleapis.com/gcc-data/Validation/GCC-1.1.0-Validation.tsv',\n",
        "      cache_subdir=data_dir,\n",
        "      cache_dir='.'\n",
        "  )\n",
        "\n",
        "  train_raw = ds_from_index_file(train_index_path, data_dir=data_dir/'train', count=num_train)\n",
        "  test_raw = ds_from_index_file(val_index_path, data_dir=data_dir/'val', count=num_val)\n",
        "\n",
        "  return train_raw, test_raw\n",
        "\n",
        "train_raw, test_raw = conceptual_captions(num_train=100, num_val=50)"
      ],
      "metadata": {
        "id": "1XCfI65XPn6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_raw.element_spec"
      ],
      "metadata": {
        "id": "Zl1dqkTeRlkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ex_path, ex_caps in train_raw.take(1):\n",
        "  print(ex_path, '\\n', ex_caps)"
      ],
      "metadata": {
        "id": "YfDJI7WKRrTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image feature extractor (PreTrained MobileNet)"
      ],
      "metadata": {
        "id": "LVsQ0kKVR4x-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SHAPE = (224, 224, 3)\n",
        "\n",
        "mobilenet = tf.keras.applications.MobileNetV3Small(\n",
        "    input_shape=IMAGE_SHAPE,\n",
        "    include_top=False,\n",
        "    include_preprocessing=True\n",
        ")\n",
        "mobilenet.trainable = False"
      ],
      "metadata": {
        "id": "ihbYMp8fRz7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image(path):\n",
        "  img = tf.io.read_file(path)\n",
        "  img = tf.io.decode_jpeg(img, channels=3)\n",
        "  img = tf.image.resize(img, IMAGE_SHAPE[:-1])\n",
        "\n",
        "  return img\n",
        "\n",
        "test_img_batch = load_image(ex_path)[tf.newaxis, :]\n",
        "print(test_img_batch.shape, '\\n', mobilenet(test_img_batch).shape)"
      ],
      "metadata": {
        "id": "oMgHRpcySJyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizer/Vectorizer"
      ],
      "metadata": {
        "id": "KBEu0-vBSgkA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def standardize(s):\n",
        "  s = tf.strings.lower(s)\n",
        "  s = tf.strings.regex_replace(s, f'[{re.escape(string.punctuation)}]', '')\n",
        "  s = tf.strings.join(['[START]', s, '[END]'], separate=' ')\n",
        "\n",
        "  return s\n",
        "\n",
        "vocab_size = 5_000  # use top 5k words\n",
        "tokenizer = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    standardize=standardize,\n",
        "    ragged=True\n",
        ")\n",
        "# kearn the vocab from captions\n",
        "tokenizer.adapt(train_raw.map(lambda fp, txt: txt).unbatch().batch(1024))\n",
        "\n",
        "t = tokenizer([['a cat in a hat'], ['a robot dog']])\n",
        "print(t)\n",
        "\n",
        "word_to_idx = tf.keras.layers.StringLookup(\n",
        "    mask_token='',\n",
        "    vocabulary=tokenizer.get_vocabulary()\n",
        ")\n",
        "idx_to_word = tf.keras.layers.StringLookup(\n",
        "    mask_token='',\n",
        "    vocabulary=tokenizer.get_vocabulary(),\n",
        "    invert=True\n",
        ")\n",
        "\n",
        "w = idx_to_word(t)\n",
        "print(w.to_list())\n",
        "print(tf.strings.reduce_join(w, separator-' ', axis=-1).numpy())"
      ],
      "metadata": {
        "id": "U0fXyMaKSXqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare dataset"
      ],
      "metadata": {
        "id": "4vhLsaF6TmmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train and test contain 1 img -> many captions, so turn it into 1:1\n",
        "def match_shapes(imgs, caps):\n",
        "  cap_shape = einops.parse_shape(caps, 'b c')\n",
        "  caps = einops.rearrange(caps, 'b c -> (b c)')\n",
        "\n",
        "  imgs = einops.repeat(\n",
        "      imgs, 'b ... -> (b c) ...',\n",
        "      c = cap_shape['c']\n",
        "  )\n",
        "\n",
        "  return imgs, caps\n",
        "\n",
        "for ex_paths, ex_captions in train_raw.batch(32).take(1):\n",
        "  print(ex_paths.shape, '\\n', ex_captions.shape)\n",
        "  ex_paths, ex_captions = match_shapes(ex_paths, ex_captions)\n",
        "  print('\\n', ex_paths.shape, '\\n', ex_captions.shape)\n",
        "\n",
        "  break\n",
        "\n",
        "\n",
        "# for keras, dataset should be (inputs, labels) pairs\n",
        "# for text gen, tokens = both input and labels, but shifted by 1 step\n",
        "def prep_txt(imgs, txts):\n",
        "  toks = tokenizer(txts)\n",
        "  inp_toks = tokens[..., :-1]\n",
        "  label_toks = tokens[..., 1:]\n",
        "\n",
        "  return (imgs, inp_toks), label_toks\n",
        "\n",
        "def prep_dataset(ds, tokenizer, batch_size=32, shuffle_buffer=1000):\n",
        "  ds = (ds\n",
        "        # load imgs, ignore those that fail\n",
        "        .shuffle(10_000).map(lambda path, cap: (load_image(path), cap))\n",
        "        .apply(tf.data.experimental.ignore_errors())\n",
        "        .batch(batch_size)\n",
        "        )\n",
        "\n",
        "  def to_tensor(inps, labels):\n",
        "    (imgs, in_tok), out_tok = inps, labels\n",
        "    return (imgs, in_tok.to_tensor()), out_tok.to_tensor()\n",
        "\n",
        "  return (ds\n",
        "          # replicate imgs to match number of captions\n",
        "          .map(match_shapes, tf.data.AUTOTUNE),\n",
        "          .unbatch()\n",
        "          # shuffle and rebatch\n",
        "          .shuffle(shuffle_buffer)\n",
        "          .batch(batch_size)\n",
        "          # tokenize and add label tokens\n",
        "          .map(prep_txt, tf.data.AUTOTUNE)\n",
        "          # convert from RaggedTensor to padded dense Tensor\n",
        "          .map(to_tensor, tf.data.AUTOTUNE)\n",
        "          )\n",
        "\n",
        "train_ds = prep_dataset(train_raw, tokenizer)\n",
        "test_ds = prep_dataset(test_raw, tokenizer)\n",
        "\n",
        "print(train_ds.element_spec, '\\n', test_ds.element_spec)"
      ],
      "metadata": {
        "id": "YTOJ0aUOThnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cache the image features (cuz MobileNet is fixed/not trainable)"
      ],
      "metadata": {
        "id": "ucr5W4ocWONg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_ds(ds, path, img_model, tokenizer, shards=10, batch_size=32):\n",
        "  ds = (ds\n",
        "        .map(lambda p, c: (load_image(p), c))\n",
        "        .apply(tf.data.experimental.ignore_errors())\n",
        "        .batch(batch_size)\n",
        "        )\n",
        "  # run feature extractor\n",
        "  def gen():\n",
        "    for (i, c) in tqdm.tqdm(ds):\n",
        "      feature_maps = img_model(i)\n",
        "      feature_maps, c = match_shapes(feature_maps, c)\n",
        "\n",
        "      yield feature_maps, c\n",
        "\n",
        "  new_ds = tf.data.Dataset.from_generator(\n",
        "      gen,\n",
        "      output_signature=(\n",
        "          tf.TensorSpec(shape=img_model.output_shape),\n",
        "          tf.TensorSpec(shape=(None,), dtype=tf.string)\n",
        "      )\n",
        "  )\n",
        "\n",
        "  new_ds = (new_ds\n",
        "            .map(prep_txt, tf.data.AUTOTUNE)\n",
        "            .unbatch()\n",
        "            .shuffle(1_000)\n",
        "            )\n",
        "\n",
        "  def shard_func(i, item):  # save dataset into shard files\n",
        "    return i % shards\n",
        "\n",
        "  new_ds.enumerate().save(path, shard_func=shard_func)\n",
        "\n",
        "def load_ds(path, batch_size=32, shuffle=1000, cycle_length=2):\n",
        "  def custom_reader_func(datasets):\n",
        "    datasets = datasets.shuffle(1_000)\n",
        "    return datasets.interleave(lambda x: x, cycle_length=cycle_length)\n",
        "\n",
        "  ds = tf.data.Dataset.load(path, reader_func=custom_reader_func)\n",
        "\n",
        "  def drop_idx(i, x): return x\n",
        "\n",
        "  ds = (ds\n",
        "        .map(drop_idx, tf.data.AUTOTUNE),\n",
        "        .shuffle(shuffle)\n",
        "        .padded_batch(batch_size)\n",
        "        .prefetch(tf.data.AUTOTUNE)\n",
        "        )\n",
        "\n",
        "  return ds\n",
        "\n",
        "save_ds(train_raw, 'train_cache', mobilenet, tokenizer)\n",
        "save_ds(test_raw, 'test_cache', mobilenet, tokenizer)"
      ],
      "metadata": {
        "id": "UsRk2JcpWJwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = load_ds('train_cache')\n",
        "test_ds = load_ds('test_cache')\n",
        "\n",
        "train_ds.element_spec"
      ],
      "metadata": {
        "id": "b5y8pfc7YNhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for (inps, ex_labels) in train_ds.take(1):\n",
        "  (ex_img, ex_in_tok) = inps\n",
        "  print(ex_img.shape, '\\n', ex_in_tok.shape, '\\n', ex_labels.shape)\n",
        "  # these are shifted by 1 step\n",
        "  print('\\n', ex_in_tok[0].numpy(), '\\n', ex_labels[0].numpy())\n",
        "  break"
      ],
      "metadata": {
        "id": "_qEztFHNYTsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer Decoder Model"
      ],
      "metadata": {
        "id": "iyHKem21YuGF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SeqEmb(tf.keras.layers.Layer):\n",
        "  def __init__(self, vocab_size, max_len, depth):\n",
        "    super().__init__()\n",
        "\n",
        "    self.pos_emb = tf.keras.layers.Embedding(input_dim=max_len, output_dim=depth)\n",
        "    # mask = True to initialize keras-masks for the model\n",
        "    self.tok_emb = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=depth, mask_zero=True)\n",
        "    self.add = tf.keras.layers.Add()\n",
        "\n",
        "  def call(self, seq):\n",
        "    # looks up embedding vec for each token\n",
        "    seq = self.tok_emb(seq)  # (batch, seq, depth)\n",
        "\n",
        "    x = tf.range(tf.shape(seq)[1])  # (seq)\n",
        "    x = x[tf.newaxis, :]  # (1, seq)\n",
        "    # looks up embedding vec for each seq location\n",
        "    x = self.pos_emb(x)  # (1, seq, depth)\n",
        "\n",
        "    # adds them\n",
        "    return self.add([seq, x])"
      ],
      "metadata": {
        "id": "nSaCfZ_pYtKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttn(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    # use this instead of + so the keras mask propagates\n",
        "    self.add = tf.keras.layers.Add()\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x):\n",
        "    attn = self.mha(query=x, value=x, use_causal_mask=True)\n",
        "    x = self.add([x, attn])\n",
        "\n",
        "    return self.layernorm(x)"
      ],
      "metadata": {
        "id": "SV3p2eFkZzcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossAttn(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    self.add = tf.keras.layers.Add()\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x, y, **kwargs):\n",
        "    attn, attn_scores = self.mha(\n",
        "        query=x, value=y,\n",
        "        return_attention_scores=True  # note this\n",
        "    )\n",
        "    self.last_attn_scores = attn_scores\n",
        "    x = self.add([x, attn])\n",
        "\n",
        "    return self.layernorm(x)"
      ],
      "metadata": {
        "id": "dmPbqSLhaLpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FFwd(tf.keras.layers.Layer):\n",
        "  def __init__(self, units, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.seq = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(units=2*units, activation='relu'),\n",
        "        tf.keras.layers.Dense(units=units),\n",
        "        tf.keras.layers.Dropout(rate=dropout_rate)\n",
        "    ])\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x):\n",
        "    # input will be (batch, seq, channels)\n",
        "    # will apply Dense pointwise across batch and seq\n",
        "    x = x + self.seq(x)\n",
        "\n",
        "    return self.layernorm(x)"
      ],
      "metadata": {
        "id": "5s0WFzToajYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, units, num_heads=1, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.self_attn = CausalSelfAttn(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=units,\n",
        "        dropout=dropout_rate\n",
        "    )\n",
        "    self.cross_attn = CrossAttn(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=units,\n",
        "        dropout=dropout_rate\n",
        "    )\n",
        "    self.ff = FFwd(units, dropout_rate)\n",
        "\n",
        "  def call(self, inps, training=False):\n",
        "    # (img, text)\n",
        "    in_seq, out_seq = inps\n",
        "\n",
        "    out_seq = self.self_attn(out_seq)\n",
        "    # cross attn uses the img\n",
        "    out_seq = self.cross_attn(out_seq, in_seq)\n",
        "    self.last_attn_scores = self.cross_attn.last_attn_scores\n",
        "\n",
        "    out_seq = self.ff(out_seq)\n",
        "\n",
        "    return out_seq"
      ],
      "metadata": {
        "id": "NQ6jyMWoa29u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output needs a Dense layer at minimum to get logit-predictions\n",
        "<br>Can improve it:\n",
        " - handle bad tokens:\n",
        "  - pad '', unknown '[UNK]', start '[START]'\n",
        "  - model should never generate these, set their bias to a large -ve value & need to ignore them in the loss function\n",
        "\n",
        "- smart init:\n",
        " - default init of Dense = initially predicts w/ almost uniform likelihood, far from the actual token dst\n",
        " - add adapt() to count the tokens and set optimal inital bias\n",
        " - reduces initial loss from entropy of uniform dst (log(vocab_size)) to marginal entropy of dst (-p*log(p))"
      ],
      "metadata": {
        "id": "yzRMupznb81n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TokOut(tf.keras.layers.Layer):\n",
        "  def __init__(self, tokenizer, banned=('', '[UNK]', '[START]'), **kwargs):\n",
        "    super().__init__()\n",
        "\n",
        "    self.dense = tf.keras.layers.Dense(\n",
        "        units=tokenizer.vocabulary_size(), **kwargs\n",
        "    )\n",
        "    self.tokenizer = tokenizer\n",
        "    self.banned = banned\n",
        "\n",
        "    self.bias = None\n",
        "\n",
        "  def adapt(self, ds):\n",
        "    counts = collections.Counter()\n",
        "    vocab_dict = {\n",
        "        name:id\n",
        "        for id, name in enumerate(self.tokenizer.get_vocabulary())\n",
        "    }\n",
        "\n",
        "    for toks in tqdm.tqdm(ds): counts.update(toks.numpy().flatten())\n",
        "\n",
        "    counts_arr = np.zeros((self.tokenizer.vocabulary_size(),))\n",
        "    counts_arr[np.array(list(counts.keys()), dtype=np.int32)] = list(counts.values())\n",
        "\n",
        "    counts_arr = counts_arr[:]\n",
        "    for tok in self.banned:\n",
        "      counts_arr[vocab_dict[tok]] = 0\n",
        "\n",
        "    total = counts_arr.sum()\n",
        "    p = counts_arr / total\n",
        "    p[counts_arr==0] = 1.0\n",
        "    log_p = np.log(p)  # log(1) = 0\n",
        "\n",
        "    entropy = -(log_p*p).sum()\n",
        "\n",
        "    print(f'\\nUniform entropy: {np.log(self.tokenizer.vocabulary_size()):0.2f}',\n",
        "          f'\\nMarginal entropy: {entropy:0.2f}')\n",
        "\n",
        "    self.bias = log_p\n",
        "    self.bias[counts_arr==0] = -1e9\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.dense(x)\n",
        "    # Add layer doesn't work cuz different shapes\n",
        "    # clears the mask, but is fine as it prevents keras from rescaling the losses\n",
        "    return x + self.bias\n",
        "\n",
        "out_layer = TokOut(tokenizer)\n",
        "out_layer.adapt(train_ds.map(lambda inps, labels: labels))"
      ],
      "metadata": {
        "id": "qVfRIOFkbzTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Captioner(tf.keras.Model):\n",
        "  def __init__(self, tokenizer, feature_extractor, out_layer, num_layers=1,\n",
        "               units=256, max_len=50, num_heads=1, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.feature_extractor = feature_extractor\n",
        "    self.tokenizer = tokenizer\n",
        "\n",
        "    self.word_to_idx = tf.keras.layers.StringLookup(mask_token='', vocabulary=tokenizer.get_vocabulary())\n",
        "    self.idx_to_word = tf.keras.layers.StringLookup(mask_token='', vocabulary=tokenizer.get_vocabulary(), invert=True)\n",
        "\n",
        "    self.seq_emb = SeqEmb(vocab_size=tokenizer.vocabulary_size, depth=units, max_len=max_len)\n",
        "    self.dec_layers = [\n",
        "        DecLayer(units, num_heads=num_heads, dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)\n",
        "    ]\n",
        "    self.out_layer = out_layer\n",
        "\n",
        "  def call(self, inps):\n",
        "    img, txt = inps\n",
        "\n",
        "    # if RGB, then apply feature extractor\n",
        "    # else assume it's already applied\n",
        "    if img.shape[-1] == 3:\n",
        "      img = self.feature_extractor(img)\n",
        "\n",
        "    # flatten\n",
        "    img = einops.rearrange(img, 'b h w c -> b (h w) c')\n",
        "\n",
        "    # if string, apply tokenizer\n",
        "    # else assume it's already applied\n",
        "    if txt.dtype == tf.string:\n",
        "      txt = tokenizer(txt)\n",
        "\n",
        "    txt = self.seq_emb(txt)\n",
        "\n",
        "    # look at the img\n",
        "    for dec_layer in self.dec_layers:\n",
        "      txt = dec_layer(inps=(img, txt))\n",
        "\n",
        "    txt = self.out_layer(txt)\n",
        "\n",
        "    return txt\n",
        "\n",
        "  # temp=0 means greedy decoding (choose most likely)\n",
        "  # temp=1 means random sampling a/c to logits\n",
        "  # temp much >> 1 means uniform random sampling\n",
        "  def simple_gen(self, img, temp=1):\n",
        "    initial = self.word_to_idx([['[START]']])  # (batch, seq)\n",
        "\n",
        "    # extract img features\n",
        "    img_features = self.feature_extractor(img[tf.newaxis, ...])\n",
        "\n",
        "    # initialize output tokens with [START]\n",
        "    toks = initial\n",
        "    for n in range(50):\n",
        "      # pass img features + tokens to the model, get logits\n",
        "      preds = self((img_features, toks)).numpy()  # (batch, seq, vocab)\n",
        "      preds = preds[:, -1, :]  # (batch, vocab)\n",
        "\n",
        "      # choose next token based on logits\n",
        "      if temp == 0:\n",
        "        next = tf.argmax(preds, axis=-1)[:, tf.newaxis]  # (batch, 1)\n",
        "      else:\n",
        "        next = tf.random.categorical(preds/temp, num_samples=1)  # (batch, 1)\n",
        "\n",
        "      # add to list of tokens and continue\n",
        "      toks = tf.concat([toks, next], axis=1)  # (batch, seq)\n",
        "\n",
        "      # end when [END] is generated\n",
        "      if next[0] == self.word_to_idx(['END']):\n",
        "        break\n",
        "\n",
        "    words = idx_to_word(toks[0, 1:-1])\n",
        "    result = tf.strings.reduce_join(words, axis=-1, separate=' ')\n",
        "\n",
        "    return result.numpy().decode()"
      ],
      "metadata": {
        "id": "GFNfmDEYeQm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Captioner(\n",
        "    tokenizer, feature_extractor=mobilenet, out_layer=out_layer,\n",
        "    units=256, dropout_rate=0.5, num_layers=2, num_heads=2\n",
        ")"
      ],
      "metadata": {
        "id": "-4XpSnh6fky6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate Captions"
      ],
      "metadata": {
        "id": "qxy-KlmYfxnL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_url = 'https://tensorflow.org/images/surf.jpg'\n",
        "img_path = tf.keras.utils.get_file('surf.jpg', origin=img_url)\n",
        "img = load_image(img_path)"
      ],
      "metadata": {
        "id": "tjtCjAvTfw5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for t in (0.0, 0.5, 1.0):\n",
        "  # model is untrained + we initialized w/ frequency of tokens\n",
        "  # so greedy output (t = 0.0) would only contain most common tokens (a, ., [END])\n",
        "  result = model.simple_gen(img, temp=t)\n",
        "  print(result)"
      ],
      "metadata": {
        "id": "47E9GJAWhBpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train"
      ],
      "metadata": {
        "id": "ejDoFQUthg1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def masked_loss(labels, preds):\n",
        "  loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels, preds)\n",
        "\n",
        "  # loss < 1e8 discards the artificial, impossibly high losses for the banned tokens\n",
        "  mask = (labels != 0) & (loss < 1e8)\n",
        "  mask = tf.cast(mask, loss.dtype)\n",
        "\n",
        "  loss = loss * mask\n",
        "  loss = tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
        "\n",
        "  return loss\n",
        "\n",
        "def masked_acc(labels, preds):\n",
        "  mask = tf.cast(labels != 0, tf.float32)\n",
        "  preds = tf.argmax(preds, axis=-1)\n",
        "  labels = tf.cast(labels, tf.int64)\n",
        "\n",
        "  matched = tf.cast(preds == labels, tf.int64)\n",
        "  acc = tf.reduce_sum(matched * mask) / tf.reduce_sum(mask)\n",
        "\n",
        "  return acc"
      ],
      "metadata": {
        "id": "hclaipvDhhab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for feedback during training\n",
        "class GenText(tf.keras.callbacks.Callback):\n",
        "  def __init__(self):\n",
        "    image_url = 'https://tensorflow.org/images/surf.jpg'\n",
        "    image_path = tf.keras.utils.get_file('surf.jpg', origin=image_url)\n",
        "    self.image = load_image(image_path)\n",
        "\n",
        "  def on_epoch_end(self, epochs=None, logs=None):\n",
        "    print('\\n\\n')\n",
        "    for t in (0.0, 0.5, 1.0):\n",
        "      result = self.model.simple_gen(self.img, temp=t)\n",
        "      print(result)\n",
        "    print('\\n')\n",
        "\n",
        "g = GenText()\n",
        "g.model = model\n",
        "g.on_epoch_end(0)"
      ],
      "metadata": {
        "id": "sPlQZK4hiP68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [\n",
        "    GenText(),\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        patience=5, restore_best_weights=True\n",
        "    )\n",
        "]"
      ],
      "metadata": {
        "id": "bGFySyYGioCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    loss=masked_loss,\n",
        "    metrics=[masked_acc]\n",
        ")"
      ],
      "metadata": {
        "id": "sxUhC5f4iw-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    # for more freq reporting, use repeate() and give values for steps\n",
        "    train_ds.repeat(),\n",
        "    steps_per_epoch=100,\n",
        "    validation_data=test_ds.repeat(),\n",
        "    validation_steps=20,\n",
        "    epochs=100,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "metadata": {
        "id": "mLZDtcM0i6x7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization"
      ],
      "metadata": {
        "id": "dClw78a8jnub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'], label='loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "\n",
        "plt.ylim([0, max(plt.ylim())])\n",
        "plt.xlabel('Epoch #')\n",
        "plt.ylabel('CE/token')\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "plt.plot(history.history['masked_acc'], label='accuracy')\n",
        "plt.plot(history.history['val_masked_acc'], label='val_accuracy')\n",
        "\n",
        "plt.ylim(0, max(plt.ylim()))\n",
        "plt.xlabel('Epoch #')\n",
        "plt.ylabel('CE/Token')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "wIyJ9zncjMx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attention Plots"
      ],
      "metadata": {
        "id": "RFD4TppMjqCB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = model.simple_gen(img, temp=0.0)\n",
        "result"
      ],
      "metadata": {
        "id": "88DKMJjqjmOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "str_toks = result.split()  # split back into tokens\n",
        "str_toks.append('[END]')\n",
        "\n",
        "# DecLayer caches attn scores for CrossAttn\n",
        "# shape: (batch=1, heads, seq, img)\n",
        "attn_maps = [layer.last_attn_scores for layer in model.dec_layers]\n",
        "print([map.shape for map in attn_maps])\n",
        "\n",
        "# stack along batch axis\n",
        "attn_maps = tf.concat(attn_maps, axis=0)\n",
        "# average over (batch, heads) axes + split image axis back into height and width\n",
        "attn_maps = einops.reduce(\n",
        "    attn_maps,\n",
        "    'batch heads seq (height width) -> seq height width',\n",
        "    height=7, width=7,\n",
        "    reduction='mean'\n",
        ")\n",
        "# have 1 map for each sequence pred\n",
        "# values in each map should sum to 1\n",
        "print(einops.reduce(attn_maps, 'seq height width -> seq', reduction='sum'))"
      ],
      "metadata": {
        "id": "pXlL7NmAjtMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_attn_maps(image, str_toks, attn_map):\n",
        "  f = plt.figure(figsize=(16, 9))\n",
        "\n",
        "  len_result = len(str_toks)\n",
        "\n",
        "  titles = []\n",
        "  for i in range(len_result):\n",
        "    map = attn_map[i]\n",
        "\n",
        "    grid_sz = max(int(np.ceil(len_result / 2)), 2)\n",
        "    ax = fig.add_subplot(3, grid_size, i+1)\n",
        "    img = ax.imshow(image)\n",
        "    ax.imshow(map, cmap='gray', alpha=0.6, extent=img.get_extent(), clim-[0.0, np.max(map)])\n",
        "\n",
        "  plt.tight_layout()\n",
        "\n",
        "plot_attn_maps(img / 255, str_toks, attn_maps)"
      ],
      "metadata": {
        "id": "U2vOkUgqkflm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# put it together\n",
        "def run_and_show_attn(model, img, temp=0.0):\n",
        "  result_txt = self.simple_gen(img, temp)\n",
        "  str_toks = result_txt.split()\n",
        "  str_toks.append('[END]')\n",
        "\n",
        "  attn_maps = [layer.last_attn_scores for layer in model.dec_layers]\n",
        "  attn_maps = tf.concat(attn_maps, axis=0)\n",
        "  attn_maps = einops.reduce(\n",
        "      attn_maps,\n",
        "      'b head s (h w) -> s h w',\n",
        "      height=7, width=7,\n",
        "      reduction='mean'\n",
        "  )\n",
        "\n",
        "  plot_attn_maps(img / 255, str_toks, attn_maps)\n",
        "\n",
        "  t = plt.suptitle(result_txt)\n",
        "  t.set_y(1.05)\n"
      ],
      "metadata": {
        "id": "Co2LdfKMlD4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_url = 'https://tensorflow.org/images/bedroom_hrnet_tutorial.jpg'\n",
        "image_path = tf.keras.utils.get_file(origin=image_url)\n",
        "image = load_image(image_path)\n",
        "\n",
        "run_and_show_attention(model, image)"
      ],
      "metadata": {
        "id": "OsYY_1-2liJt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}